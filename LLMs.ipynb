{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kenuuey/AI-tools/blob/main/LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v16Rdji5wJnI"
      },
      "source": [
        "### Mini Orca 3B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaGPuCpswKiE",
        "outputId": "4dd608c5-d1e5-4ad3-9860-c07a6ddc91db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -qqq langchain transformers accelerate sentencepiece gpt4all\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fv_yaPDqwPo9"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./models/mini_orca_3b\n",
        "\n",
        "# Download model weights (3 safetensors files + index)\n",
        "!wget -O ./models/mini_orca_3b/model-00001-of-00003.safetensors \"https://huggingface.co/pankajmathur/orca_mini_3b/resolve/main/model-00001-of-00003.safetensors\"\n",
        "!wget -O ./models/mini_orca_3b/model-00002-of-00003.safetensors \"https://huggingface.co/pankajmathur/orca_mini_3b/resolve/main/model-00002-of-00003.safetensors\"\n",
        "!wget -O ./models/mini_orca_3b/model-00003-of-00003.safetensors \"https://huggingface.co/pankajmathur/orca_mini_3b/resolve/main/model-00003-of-00003.safetensors\"\n",
        "!wget -O ./models/mini_orca_3b/model.safetensors.index.json \"https://huggingface.co/pankajmathur/orca_mini_3b/resolve/main/model.safetensors.index.json\"\n",
        "\n",
        "# Download tokenizer\n",
        "!wget -O ./models/mini_orca_3b/tokenizer.model \"https://huggingface.co/pankajmathur/orca_mini_3b/resolve/main/tokenizer.model\"\n",
        "\n",
        "# Download configuration files\n",
        "!wget -O ./models/mini_orca_3b/config.json \"https://huggingface.co/pankajmathur/orca_mini_3b/resolve/main/config.json\"\n",
        "!wget -O ./models/mini_orca_3b/generation_config.json \"https://huggingface.co/pankajmathur/orca_mini_3b/resolve/main/generation_config.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b1SUfZxwgjN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_path = \"./models/mini_orca_3b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnem_ydlwlAn",
        "outputId": "67a41a4d-48d8-4fb6-c3fd-928ef2fb9469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a short 2-line poem about AI.\n",
            "AI, still learning, always striving to improve.\n",
            "AI, vast and complex, still a mystery.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Write a short 2-line poem about AI.\"\n",
        "output = [generator](prompt, max_new_tokens=100)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGVx1M5gwm4z",
        "outputId": "38bc566a-252f-4897-bbfb-889ff0c90ba6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3324905671.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n",
            "/tmp/ipython-input-3324905671.py:5: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm(\"Explain LangChain in one sentence.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explain LangChain in one sentence.\n",
            "LangChain is a blockchain-based platform that enables secure and transparent communication between language partners.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "response = llm(\"Explain LangChain in one sentence.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssqcRxgfxwC2",
        "outputId": "d0cd03bc-f4d6-4499-b56c-8fea335889cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summarize this text in one sentence: LangChain is a Python framework that allows you to build applications using large language models easily.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-540201409.py:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "/tmp/ipython-input-540201409.py:11: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  summary = chain.run(\"LangChain is a Python framework that allows you to build applications using large language models easily.\")\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Define a prompt template\n",
        "template = \"Summarize this text in one sentence: {text}\"\n",
        "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
        "\n",
        "# Connect the prompt and LLM\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "summary = chain.run(\"LangChain is a Python framework that allows you to build applications using large language models easily.\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxtkdFgs4gT9"
      },
      "source": [
        "### OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snN5nnGliN-h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyvKe6buhsLP"
      },
      "outputs": [],
      "source": [
        "# os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key:')\n",
        "# os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZOb7HAPiqmp"
      },
      "outputs": [],
      "source": [
        "# from openai import OpenAI\n",
        "# client = OpenAI()\n",
        "\n",
        "# response = client.responses.create(\n",
        "#     model=\"gpt-5\",\n",
        "#     input=\"Write a short bedtime story about a unicorn.\"\n",
        "# )\n",
        "\n",
        "# print(response.output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unSUSdYigiNE"
      },
      "outputs": [],
      "source": [
        "# from langchain.llms import OpenAI\n",
        "\n",
        "# llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# response = llm(\"Write a short poem about AI in 2 lines.\")\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ibNKhe4gi9m"
      },
      "outputs": [],
      "source": [
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.chains import LLMChain\n",
        "\n",
        "# template = \"Summarize the following text in one sentence:\\n{text}\"\n",
        "# prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
        "\n",
        "# chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# text = \"LangChain helps you connect LLMs to APIs, databases, and workflows.\"\n",
        "# summary = chain.run(text)\n",
        "# print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lCXxSYwhJ43"
      },
      "outputs": [],
      "source": [
        "# from langchain.memory import ConversationBufferMemory\n",
        "# from langchain.chains import ConversationChain\n",
        "\n",
        "# memory = ConversationBufferMemory()\n",
        "\n",
        "# conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "# print(conversation.run(\"Hi, who are you?\"))\n",
        "# print(conversation.run(\"Can you summarize our chat so far?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5xrZwZzhNoE"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "def add_numbers(a: str, b: str):\n",
        "    return str(int(a) + int(b))\n",
        "\n",
        "tools = [Tool(name=\"Adder\", func=add_numbers, description=\"Adds two numbers\")]\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "result = agent.run(\"Add 12 and 30\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrpGog6DwKz_"
      },
      "source": [
        "### Rand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sOnMCy8WfmN9"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq langchain langchain_community gpt4all\n",
        "!pip install -qqq huggingface_hub transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nNQEp1iWvgOG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_path = \"./models/mini_orca_3b\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\")\n",
        "\n",
        "# # Create a text-generation pipeline\n",
        "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cuoy7czetlbF"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"pankajmathur/orca_mini_3b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1O_zqDxwAzT",
        "outputId": "2bc009d7-548d-4703-d4a3-7d33c4e14bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a short 2-line poem about AI.\n",
            "AI is a machine, so complex and bright.\n",
            "It learns and adapts, and can do much more.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Write a short 2-line poem about AI.\"\n",
        "output = pipe(prompt, max_new_tokens=100)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zt82yIKNtAGF"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import GPT4All\n",
        "\n",
        "# Load the downloaded model\n",
        "llm = GPT4All(model=\"./mini_orca_q4_0.gguf\")\n",
        "\n",
        "# Test it\n",
        "response = llm(\"Write a short 2-line poem about AI.\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMa+CikAfiA7na6Ft4h+QpJ",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1GNsIavLArUmfqjxXScr3-O4YBJb53cVE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
